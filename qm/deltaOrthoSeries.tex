%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

%
%
%\documentclass{article}

%\input{../peeters_macros.tex}
%\input{../peeters_macros2.tex}

%\usepackage{txfonts} % for ointctr... (also appears to make "prettier" \int and \sum's)

%\usepackage[bookmarks=true]{hyperref}

%\usepackage{color,cite,graphicx}
   % use colour in the document, put your citations as [1-4]
   % rather than [1,2,3,4] (it looks nicer, and the extended LaTeX2e
   % graphics package.
%\usepackage{latexsym,amssymb,epsf} % do not remember if these are
   % needed, but their inclusion can not do any damage


\chapter{Dirac delta function in terms of orthogonal functions}
\label{chap:deltaOrthoSeries}
%\author{Peeter Joot \quad peeterjoot@protonmail.com }
\date{ March 8, 2009.  deltaOrthoSeries.tex }

%\begin{document}

%\maketitle{}
%\tableofcontents

\section{Motivation}

Chapter II of \citep{pauli2000wm} expresses the delta function in terms of
orthonormal basis functions, but the treatment is slightly
hard to follow.
Re-express some of this in my own words the slow and dumb way to get an
understanding of the ideas.  Also explore the summation representation of
the delta function and use it to relate Fourier series and transforms.

\section{Fourier coefficients}

Given an orthonormal basis

\begin{equation}\label{eqn:deltaOrthoSeries:20}
\begin{aligned}
\int u_m^\conj(x) u_n(x) = \delta_{mn}
\end{aligned}
\end{equation}

For a function that can be expressed entirely in this basis, such as

\begin{equation}\label{eqn:deltaOrthoSeries:40}
\begin{aligned}
f(x) = \sum_k a_k u_k(x)
\end{aligned}
\end{equation}

We can then compute the Fourier coefficients \(a_k\) in the normal fashion

\begin{equation}\label{eqn:deltaOrthoSeries:60}
\begin{aligned}
\int u_k^\conj(x) f(x) dx
&= \sum_n a_n \int u_k^\conj(x) u_n(x) dx \\
&= \sum_n a_n \delta_{kn} \\
&= a_k \\
\end{aligned}
\end{equation}

So we have
\begin{equation}\label{eqn:deltaOrthoSeries:80}
\begin{aligned}
f(x) = \sum_k a_k u_k(x)  = \sum_k u_k(x) \int u_k^\conj(x') f(x') dx'
\end{aligned}
\end{equation}

\subsection{Mean square convergence}

How good of a match is a subset of such a sum?  Pauli considers a mean convergence.

\begin{equation}\label{eqn:deltaOrthoSeries:100}
\begin{aligned}
0 &= \lim_{N \rightarrow \infty}\int
{\Abs{f(x') -\sum_{k=1}^N a_k u_k(x') }}^2 dx'  \\
&=
\int \left(f^\conj(x') -\sum_{k=1}^N a_k^\conj u_k^\conj(x') \right) \left(f(x') - \sum_{m=1}^N a_m u_m(x') \right)
dx' \\
&=
\int
\left( f^\conj(x') f(x')
-f^\conj(x') \sum_{m=1}^N a_m u_m(x')
- \sum_{k=1}^N a_k^\conj u_k^\conj(x') f(x')
+ \sum_{m=1}^N a_m u_m(x') \sum_{k=1}^N a_k^\conj u_k^\conj(x')  \right)
dx' \\
&=
\int f^\conj(x') f(x') dx'
- \sum_{m=1}^N a_m a_m^\conj
- \sum_{k=1}^N a_k^\conj a_k
+ \sum_{m=1}^N \sum_{k=1}^N a_m a_k^\conj \delta_{km} \\
%\int u_m(x') u_k^\conj(x') dx' \\
&= \int \Abs{f(x')}^2 dx' - \sum_{m=1}^N \Abs{a_m}^2 \\
\end{aligned}
\end{equation}

So if we have mean square equality in the limit as \(N \rightarrow \infty\), then it must also be true that

\begin{equation}\label{eqn:deltaOrthoSeries:120}
\begin{aligned}
\int \Abs{f(x')}^2 dx' = \sum_{m=1}^\infty \Abs{a_m}^2 \\
\end{aligned}
\end{equation}

He calls this the completeness relation.  If the orthonormal basis is sufficient to express the set of desired functions, then
the squared absolute value of such functions can be expressed entirely in terms of the Fourier coefficients.  The mean square
equality is weaker in the sense that a function can be mismatched to its Fourier representation at a set (of ``measure zero'') points,
and still meet the mean square equality statement.

\subsection{Generalizing the inner product}

Pauli next introduces the an inner product on functions (without calling it that)
in a somewhat indirect
fashion (ie: in terms of Fourier components instead of by definition).

Supposing that one has two functions built up by Fourier components

\begin{equation}\label{eqn:deltaOrthoSeries:140}
\begin{aligned}
f(x) &= \sum_k a_k u_k(x) \\
g(x) &= \sum_k b_k u_k(x) \\
\end{aligned}
\end{equation}

Then we have
\begin{equation}\label{eqn:deltaOrthoSeries:160}
\begin{aligned}
\int f^\conj(x) g(x) &= \sum_{k,m} a_k^\conj b_m \int u_k^\conj(x) u_m(x) = \sum_k a_k^\conj b_k \\
\int g^\conj(x) f(x) &= \sum_{k,m} a_k b_m^\conj \int u_m^\conj(x) u_k(x) = \sum_k b_k^\conj a_k \\
\end{aligned}
\end{equation}

This is something that is familiar to anybody who has taken a linear
algebra course, but perhaps had to be motivated when he wrote the book?

\subsection{Delta function as a sum}

Perhaps Pauli wrote this general function inner product that way to show a natural way that a sum of the
form

\begin{equation}\label{eqn:deltaOrthoSeries:180}
\begin{aligned}
\sum u_m^\conj(x) u_k(x)
\end{aligned}
\end{equation}

arises in use, because he now writes the completeness relation using a sum similar to that above

\begin{equation}\label{eqn:delta_ortho_series:deltaSum}
\begin{aligned}
\sum_{k} u_k^\conj(x') u_k(x) \equiv \delta(x-x')
\end{aligned}
\end{equation}

I had seen this in bra ket notation, in Susskind's lectures as noted in \chapcite{PJQmSusskind}, and also in \citep{mcmahon2005qmd} as the
identity operator

\begin{equation}\label{eqn:deltaOrthoSeries:200}
\begin{aligned}
\sum_{k} \ketbra{k}{k} \equiv \delta(x-x')
\end{aligned}
\end{equation}

From neither of those two sources did I understand where it came from (in Susskind's lectures it appeared to be
related to Fourier transforms).
As Pauli did, let us verify that this works, and try to relate this to a few specific choices of inner products (covering at
least classical Fourier series and the Fourier transform).

The relation of \eqnref{eqn:delta_ortho_series:deltaSum} can be shown to have delta function behavior by integration

\begin{equation}\label{eqn:deltaOrthoSeries:220}
\begin{aligned}
\int \sum_{k} u_k^\conj(x') u_k(x) f(x') dx'
&=
\sum_{k,m} u_k(x) a_m \int u_k^\conj(x') u_m(x') dx' \\
&=
\sum_{k,m} u_k(x) a_m \delta_{km} \\
&=
\sum_{k} u_k(x) a_k \\
&=
f(x)
\end{aligned}
\end{equation}

Strictly speaking this ought to be formulated in terms of mean square convergence since an arbitrary function f(x)
may differ from its Fourier sum at specific points (for example at points of discontinuity).

\subsubsection{Fourier series example}

Suppose the inner product is defined for the range \(I = [a, a+T]\).

\begin{equation}\label{eqn:deltaOrthoSeries:240}
\begin{aligned}
\Innerprod{f}{g} &= \int_{\partial I} f^\conj(x) g(x) dx
\end{aligned}
\end{equation}

What is the identity operator representation in the Fourier series basis \({u'}_k(x) = e^{ 2 \pi i k x / T}\)?  First the
normalization is required.

\begin{equation}\label{eqn:deltaOrthoSeries:260}
\begin{aligned}
\Innerprod{{u'}_k}{{u'}_m}
&= \int_{\partial I} e^{ 2 \pi i (m-k) x /T } dx  \\
&= \delta_{km} T
\end{aligned}
\end{equation}

So our orthonormalized basis is

\begin{equation}\label{eqn:deltaOrthoSeries:280}
\begin{aligned}
u_k(x) = \inv{\sqrt{T}} e^{ 2 \pi i k x / T}
\end{aligned}
\end{equation}

Given this orthonormal basis we can write

\begin{equation}\label{eqn:deltaOrthoSeries:300}
\begin{aligned}
f(x)
&= \sum_k a_k u_k(x) \\
a_k &= \int_{\partial I} u_k^\conj(x) f(x) dx = \Innerprod{u_k(x)}{f(x)} \\
\end{aligned}
\end{equation}

Or in a vector like notation

\begin{equation}\label{eqn:deltaOrthoSeries:320}
\begin{aligned}
f(x) &= \sum_k u_k(x) \Innerprod{u_k(x)}{f(x)}
\end{aligned}
\end{equation}

In this basis the
delta function (identity operator) form of \eqnref{eqn:delta_ortho_series:deltaSum}
becomes

\begin{equation}\label{eqn:deltaOrthoSeries:340}
\begin{aligned}
\delta(x- x') = \inv{{T}} \sum_k e^{ 2 \pi i k (x-x') / T}
\end{aligned}
\end{equation}

\subsubsection{Fourier transform inner-product}

For the Fourier transform we have an infinite range inner product

\begin{equation}\label{eqn:deltaOrthoSeries:360}
\begin{aligned}
\Innerprod{f}{g} &= \IIinf f^\conj(x) g(x) dx
\end{aligned}
\end{equation}

With a Fourier transform pair

\begin{equation}\label{eqn:deltaOrthoSeries:380}
\begin{aligned}
\hat{f}(k) &= \frac{1}{\sqrt{2\pi}} \int f(x) e^{-i k x} dx \\
{f}(x) &= \frac{1}{\sqrt{2\pi}} \int \hat{f}(k) e^{i k x } dk \\
\end{aligned}
\end{equation}

It appears that a natural choice of basis functions is actually \(u_k\) from the
Fourier series above with \(T=2\pi\).  That is

\begin{equation}\label{eqn:deltaOrthoSeries:400}
\begin{aligned}
u_k = \inv{\sqrt{2\pi}} e^{i k x}
\end{aligned}
\end{equation}

Our Fourier coefficients are now continuous and we have a form that
is very close to the discrete Fourier series

\begin{equation}\label{eqn:deltaOrthoSeries:420}
\begin{aligned}
f(x)
&= \int dk a_k u_k \\
a_k &= \int u_k^\conj(x) f(x) dx = \Innerprod{u_k(x)}{f(x)} \\
\end{aligned}
\end{equation}

Besides the inner product range difference from the discrete frequency case
the only other difference in this formulation is that we have a
\(\sum_k \rightarrow \int dk\) replacement.

What is the delta function representation in this inner product space?

A continuous variation of the summation delta function representation
in the Fourier series basis is

\begin{equation}\label{eqn:deltaOrthoSeries:440}
\begin{aligned}
\int dk u_k^\conj(x) u_k(x')
&=
\int dk \inv{2\pi} e^{ i k (x' - x)}
\end{aligned}
\end{equation}

Okay, cool.  The principle value of this integral is the sinc function
that is the familiar limiting form of the delta function.

This is an interesting and unifying way of expressing these Fourier
relationships.  The inner product is seen here to provide a more general
structure that is common to both the Fourier series and Fourier transform.
It is not surprising that the physicists rightly pick the algebraic
orthonormal function representation as fundamental ... too bad they do it
all with the braket notation that automatically obfuscates the subject.
%(somebody like me familiar with inner product spaces still can not look at
%a QM book and without wondering what sort of drug needs to be splook
%at a only they know anything about.

This also clarifies for me what Susskind did in his QM lectures.  There
he used the identity operator representation to express the Fourier transform
without ever touching on the tricky aspects of Fourier inversion.  That is a
tricky but interesting approach.

\subsubsection{Legendre polynomials}

Let us see how one non-Fourier like inner product function space
representation works out this way.

Using the Legendre inner product

\begin{equation}\label{eqn:deltaOrthoSeries:460}
\begin{aligned}
\Innerprod{f}{g} &= \int_{-1}^1 f(x) g(x) dx
\end{aligned}
\end{equation}

An orthonormal basis can be had by normalizing the
Legendre polynomials.
\href{ http://mathworld.wolfram.com/LegendrePolynomial.html }{Wolfram's Legendre Polynomial page} lists these in a number of closed forms

\begin{equation}\label{eqn:deltaOrthoSeries:480}
\begin{aligned}
P_n(x)
&= \inv{2 \pi i} \ointctrclockwise \frac{dt}{t^{n+1}\sqrt{1 - 2t x +t^2}} \\
&= \inv{2^n}\sum_{k=0}^n {\binom{n}{k}}^2 (x-1)^{n-k}(x+1)^k
\end{aligned}
\end{equation}

The first of these uses a closed contour around the origin.

These polynomials are not orthonormal, having
\begin{equation}\label{eqn:deltaOrthoSeries:500}
\begin{aligned}
\Innerprod{P_n}{P_m} &= \frac{2}{2 n + 1}\delta_{mn}
\end{aligned}
\end{equation}

So we have an orthonormal basis if we pick
\begin{equation}\label{eqn:deltaOrthoSeries:520}
\begin{aligned}
u_n(x) &= P_n(x) \sqrt{n + 1/2}
\end{aligned}
\end{equation}

Our delta function representation in this basis becomes

\begin{equation}\label{eqn:deltaOrthoSeries:540}
\begin{aligned}
\delta(x-x')
&\sim \sum_{n=0}^\infty \left(n+ \inv{2}\right) P_n(x') P_n(x) \\
&= -\inv{4\pi^2} \sum_{n=0}^\infty \left(n+ \inv{2}\right) \ointctrclockwise \frac{du}{u^{n+1}\sqrt{1 - 2u x' + u^2}} \ointctrclockwise \frac{dt}{t^{n+1}\sqrt{1 - 2t x + t^2}} \\
&= \sum_{n=0}^\infty \sum_{m=0}^n \sum_{k=0}^n \frac{n+ \inv{2}}{2^{2n}} {\binom{n}{m}}^2 (x'-1)^{n-m}(x'+1)^m {\binom{n}{k}}^2 (x-1)^{n-k}(x+1)^k
\end{aligned}
\end{equation}

Neither of these are familiar looking to me, but I was mostly curious to see one of these delta representations for a non-Fourier-ish basis.  A number of other
orthogonal polynomials can be found detailed in \href{http://mathworld.wolfram.com/OrthogonalPolynomials.html}{Wolfram's orthogonal polynomial article.}

%\bibliographystyle{plainnat}
%\bibliography{myrefs}

%\end{document}
