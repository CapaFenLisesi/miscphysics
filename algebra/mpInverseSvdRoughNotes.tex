%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

%
%
\chapter{Singular Value Decomposition}
\label{chap:mpInverseSvdRoughNotes}
\date{ May 15, 2008.  mpInverseSvdRoughNotes.tex }

\section{blah}

\subsection{Application of projection as left pseudoinverse (ie: linear fitting)}

%We have shown that the left pseudoinverse product with the matrix can
%be expressed as a projection matrix (sum of the projection matrices
%associated with a set of orthonormal vectors)

%\begin{equation}\label{eqn:mpInvRough:pseudoprojmatsum}
%A^{+} A =
%\sum_{k=1}^r {u_k}u_k^\T
%\end{equation}
%

%(note this is a different ``\(V\)'' than the \(V\) in \(A = U \Sigma V^\T\) since it only includes the first \(r\) columns).
%This allows us to write the matrix of \eqnref{eqn:mpInvRough:pseudoprojmatsum} as

%\begin{equation}
%A^{+} A = V V^\T
%\end{equation}

Equation (FIXME)
%\eqnref{eqn:mpInvRough:projectiongeneralmatrix} %% ??
provides us a way to find best solutions to general equations of the form:


\begin{equation}\label{eqn:mpInverseSvdRoughNotes:20}
A x = b
\end{equation}

Here \(A\) is the matrix of a linear transformation, \(A : \mathbb{R}^k \rightarrow \mathbb{R}^n\), for some \(k<n\).
By ``best solutions'' here, we give this the geometrical meaning, namely, the solution matching the projection of \(b\) onto the space.

If b is not completely in the column space \(C(A)\) of \(A\), this can have no solution.  However, writing

\begin{equation}\label{eqn:mpInverseSvdRoughNotes:40}
b = \Proj_A(b) + b_\perp
\end{equation}

as the components of \(b\) in \(C(A)\) and not in \(C(A)\) respectively we can at least solve the reduced equation for \(\hat{x}\):


\begin{equation}\label{eqn:mpInvRough:reducedinverseproblem}
A \hat{x} = \Proj_A(b)
\end{equation}


This will be possible even in circumstances that the original equation had no solution.  Specifically, the vector b when projected onto the plane can be expressed as some
linear combination of the columns of \(A\) (a basis for the subspace).

Substitution of our projection result into \eqnref{eqn:mpInvRough:reducedinverseproblem} yields:

\begin{equation}\label{eqn:mpInverseSvdRoughNotes:180}
\begin{aligned}
A \hat{x}
&= \Proj_{A}\left(b\right) = A (A^\T A)^{-1} A^\T b
\end{aligned}
\end{equation}

The simplest case here is when \(A\) is of full column rank since one can pre-multiply this complete equation by \(A^\T\) without any possibility of nulling
\(A \hat{x}\).

\begin{equation}\label{eqn:mpInverseSvdRoughNotes:200}
\begin{aligned}
A^\T A \hat{x}
&= A^\T A (A^\T A)^{-1} A^\T b \\
&= A^\T b \\
\end{aligned}
\end{equation}

Thus our best fit vector is

\begin{equation}
\hat{x}
= (A^\T A)^{-1} A^\T b
\end{equation}

Another way to view this is for any vector \(x\) that is not in the null space \(N(A)\), then the matrix:

\begin{equation}
A^{+}= (A^\T A)^{-1} A^\T
\end{equation}

has the action of a left inverse for any full column rank matrix \(A\).  Thus when there is a solution to:

\begin{equation}
A x = b.
\end{equation}

It can be obtained by pre-multiplication using this "left" inverse.

\begin{equation}
A^{+} A x = x = A^{+} b
\end{equation}











































\section{SVD connection}


SVT decomposition is an factoring of \(A \in M^{m \times n}\) with orthonormal matrices \(U \in M^{m \times m}\)

and \(V \in M^{n \times n} \) producing the following form:

\begin{equation}\label{eqn:mpInverseSvdRoughNotes:60}
A = U \Sigma V^\T
\end{equation}

Sigma has the form:

\begin{equation}\label{eqn:mpInverseSvdRoughNotes:80}
\Sigma =
\begin{bmatrix}
D_{r,r} & 0_{r,n-r} \\
0_{m-r,r} & 0_{m-r,n-r} \\
\end{bmatrix}
\end{equation}

where \(r = \rank(A)\), and \(D\) is a diagonal matrix with the root of the (positive) eigenvalues of \(A^\T A\).

This provides a generalized spectral decomposition and similarity that applies to both non-square matrices and matrices not otherwise diagonalizable
(ie: square matrix with similarity to a Jordon form matrix).  Given this decomposition we can write:

\begin{equation}\label{eqn:mpInverseSvdRoughNotes:100}
\Sigma = U^\T A V
\end{equation}

If one were to ask the question of what is the closest that one could get to inverting such a matrix.  It is pretty clear that the closest one could get to
identity will be with multiplication of a \(\Sigma^{+}\) of the following form:

\begin{equation}\label{eqn:mpInverseSvdRoughNotes:120}
\Sigma^{+} \Sigma
=
\begin{bmatrix}
(D_{r,r})^{-1} & 0_{r,m-r} \\
0_{n-r,r} & 0_{n-r,m-r} \\
\end{bmatrix}
\begin{bmatrix}
D_{r,r} & 0_{r,n-r} \\
0_{m-r,r} & 0_{m-r,n-r} \\
\end{bmatrix}
=
\begin{bmatrix}
I_{r,r} & 0_{r,n-r} \\
0_{n-r,r} & 0_{n-r,n-r} \\
\end{bmatrix}
\end{equation}

For a right pseudoinverse we have a similar result:

\begin{equation}\label{eqn:mpInverseSvdRoughNotes:140}
\Sigma
\Sigma^{+}
=
\begin{bmatrix}
D_{r,r} & 0_{r,n-r} \\
0_{m-r,r} & 0_{m-r,n-r} \\
\end{bmatrix}
\begin{bmatrix}
(D_{r,r})^{-1} & 0_{r,m-r} \\
0_{n-r,r} & 0_{n-r,m-r} \\
\end{bmatrix}
=
\begin{bmatrix}
I_{r,r} & 0_{r,m-r} \\
0_{m-r,r} & 0_{m-r,m-r} \\
\end{bmatrix}
\end{equation}

With either of these one can define a corresponding pseudoinverse (left or right) as:

\begin{equation}
A^{+} = V \Sigma^{+} U^\T
\end{equation}

This is a logical definition, but how close is it to the projective
left inverse we calculated above in the case where \(A\) is not of full column
rank?

Multiplication gives:

\begin{equation}\label{eqn:mpInverseSvdRoughNotes:220}
\begin{aligned}
A^{+} A
&= V \Sigma^{+} U^\T U \Sigma V^\T \\
&= V \Sigma^{+} \Sigma V^\T \\
&=
\begin{bmatrix}
v_1 & v_2 & \cdots & v_r & v_{r+1} & \cdots & v_n \\
\end{bmatrix}
\begin{bmatrix}
(D_{r,r})^{-1} & 0_{r,m-r} \\
0_{n-r,r} & 0_{n-r,m-r} \\
\end{bmatrix}
\begin{bmatrix}
D_{r,r} & 0_{r,n-r} \\
0_{m-r,r} & 0_{m-r,n-r} \\
\end{bmatrix}
\begin{bmatrix}
v_1^\T \\ v_2^\T \\ \vdots \\ v_r^\T \\ {v_{r+1}}^\T \\ \vdots \\ v_n^\T \\
\end{bmatrix}
\end{aligned}
\end{equation}
%Embeded in that is the same "as-close-to" identity as calculated above.

Writing \(D_{r,r} = [\delta_{ij}\sigma_i]_{ij}\), we have:

\begin{equation}\label{eqn:mpInvRough:VIrVt}
V \Sigma^{+} \Sigma V^\T
=
\begin{bmatrix}
\frac{v_1}{\sigma_1} & \frac{v_2}{\sigma_2} & \cdots & \frac{v_r}{\sigma_2} & 0 & \cdots & 0
\end{bmatrix}
\begin{bmatrix}
v_1^\T \sigma_1 \\ v_2^\T \sigma_2 \\ \vdots \\ v_r^\T \sigma_r \\ 0 \\ \vdots \\ 0 \\
\end{bmatrix}
\end{equation}

Considering this as the product of block matrices we have a product here of the form

\begin{equation}\label{eqn:mpInverseSvdRoughNotes:240}
\begin{aligned}
\begin{bmatrix}
A_{n,r} & 0_{n,n-r}
\end{bmatrix}
\begin{bmatrix}
B_{r,n} \\ 0_{n-r,n}
\end{bmatrix} \\
&=
\begin{bmatrix}
A_{n,r} B_{r,n} + 0_{n,n-r} 0_{n-r,n}
\end{bmatrix} \\
&=
\begin{bmatrix}
A_{n,r} B_{r,n} + 0_{n,n}
\end{bmatrix} \\
&=
\begin{bmatrix}
A_{n,r} B_{r,n}
\end{bmatrix}
\end{aligned}
\end{equation}

Thus we can strip the block zero matrices from \eqnref{eqn:mpInvRough:VIrVt} and write

\begin{equation}\label{eqn:mpInvRough:pseudoinversetimesmatrix}
A^{+} A =
V \Sigma^{+} \Sigma V^\T
=
\begin{bmatrix}
\frac{v_1}{\sigma_1} & \frac{v_2}{\sigma_2} & \cdots & \frac{v_r}{\sigma_2}
\end{bmatrix}
\begin{bmatrix}
v_1^\T \sigma_1 \\ v_2^\T \sigma_2 \\ \vdots \\ v_r^\T \sigma_r
\end{bmatrix}
\end{equation}

Eliminating the \(\sigma\) terms we have:

\begin{equation}\label{eqn:mpInvRough:pseudoinversetimesmatrixsum}
A^{+} A =
\begin{bmatrix}
\sum_{k=1}^r {v_k}v_k^\T
\end{bmatrix}
=
\begin{bmatrix}
v_1 & v_2 & \cdots & v_r
\end{bmatrix}
\begin{bmatrix}
v_1^\T \\ v_2^\T \\ \vdots \\ v_r^\T
\end{bmatrix}
\end{equation}

We previously calculated a left inverse using the projection matrix associated with a full column rank matrix.  For this product to have the properties of a
left acting inverse we also expect it to be a projection.
Let us digress
slightly before looking at whether equation
\eqnref{eqn:mpInvRough:pseudoinversetimesmatrixsum} satisfies this expectation.

\subsection{Correlating the SVD derived projection matrix back to \texorpdfstring{\(A\)}{A}}

We now have to show that this is also the projection matrix associated

with the columns of the
original matrix that we have an SVD factorization for

\begin{equation}\label{eqn:mpInverseSvdRoughNotes:160}
A = U \Sigma V^\T
\end{equation}

Once we show this, then we have also demonstrated that the first \(r\)
(orthonormal) column vectors in the matrix \(V\) of this decomposition
are a basis for the column space of \(A\) itself.  Note that we are
switching back to the original definition of \(V \in M^{n,n}\) here, and
not the \(V \in M^{n,r}\) of equation (FIXME)
%\eqnref{eqn:mpInvRough:projOrthonormal}. % ???
